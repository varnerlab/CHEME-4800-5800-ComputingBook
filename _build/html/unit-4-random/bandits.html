
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Bandit Problems and Reinforcement Learning &#8212; CHEME 4800/5800</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="../_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=36754332"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-VQRVBL1C02"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-VQRVBL1C02');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-VQRVBL1C02');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'unit-4-random/bandits';</script>
    <link rel="canonical" href="https://varnerlab.github.io/CHEME-1800-Computing-Book/landing.html/unit-4-random/bandits.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="References" href="../References.html" />
    <link rel="prev" title="Markov Chains and Markov Decision Processes" href="markov.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../landing.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/cornell_seal_simple_black-164.svg" class="logo__image only-light" alt="CHEME 4800/5800 - Home"/>
    <script>document.write(`<img src="../_static/cornell_seal_simple_black-164.svg" class="logo__image only-dark" alt="CHEME 4800/5800 - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../landing.html">
                    Principles of Computational Thinking for Engineers
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../unit-1-basics/basics-landing.html">Unit 1. Computing Basics</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../unit-1-basics/types.html">Expressions, Variables, and Types</a></li>

<li class="toctree-l2"><a class="reference internal" href="../unit-1-basics/functions.html">Functions, Control Statements, Recursion and Testing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../unit-1-basics/programs.html">Programs and Modules</a></li>
<li class="toctree-l2"><a class="reference internal" href="../unit-1-basics/data-file-io.html">Data Input and Output</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../unit-2-data/data-landing.html">Unit 2. Data Structures</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../unit-2-data/trees.html">Data Structures</a></li>
<li class="toctree-l2"><a class="reference internal" href="../unit-2-data/vectors-matricies-nla.html">Vectors, Matrices and Linear Algebraic Equations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../unit-2-data/reduction.html">Dimensionality Reduction, Eigenvalues, and Eigenvectors</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../unit-3-learning/learning-landing.html">Unit 3. Optimization</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../unit-3-learning/penalty.html">Ordinary Least Squares Problems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../unit-3-learning/lp.html">Linear Programming</a></li>
<li class="toctree-l2"><a class="reference internal" href="../unit-3-learning/combitorial.html">Dynamic programming and Heuristic Optimization</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="random-landing.html">Unit 4. Random Processes</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="markov.html">Markov Chains and Markov Decision Processes</a></li>

<li class="toctree-l2 current active"><a class="current reference internal" href="#">Bandit Problems and Reinforcement Learning</a></li>

</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../References.html">References</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/varnerlab/CHEME-1800-Computing-Book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/varnerlab/CHEME-1800-Computing-Book/issues/new?title=Issue%20on%20page%20%2Funit-4-random/bandits.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/unit-4-random/bandits.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Bandit Problems and Reinforcement Learning</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Bandit Problems and Reinforcement Learning</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bandit-problems">Bandit problems</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bandit-problem-formulation">Bandit problem formulation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-concepts-of-reinforcement-learning">Basic concepts of reinforcement learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-free-reinforcement-learning">Model-free reinforcement learning</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#q-learning">Q-learning</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="bandit-problems-and-reinforcement-learning">
<h1>Bandit Problems and Reinforcement Learning<a class="headerlink" href="#bandit-problems-and-reinforcement-learning" title="Link to this heading">#</a></h1>
<aside class="topic">
<p class="topic-title">Outline</p>
<p>In this lecture, we will delve into the problem, covering different algorithms and strategies for balancing exploration and exploitation, and explore the following topics:</p>
<ul class="simple">
<li><p><a class="reference internal" href="#content-references-multiarm-bandit-problems"><span class="std std-ref">Bandit problems</span></a> refer to a scenario where an agent must repeatedly choose between multiple actions, each with an unknown reward distribution. The goal of the agent is to maximize its total reward over time while learning the reward distributions of the different actions. This presents a tradeoff between exploration, where the agent chooses the action with the highest estimated reward so far, and exploitation, where it selects an action with an uncertain reward to learn more about its distribution.</p></li>
<li><p><a class="reference internal" href="#content-references-basic-concepts-reinforcement-learning"><span class="std std-ref">Reinforcement Learning (RL)</span></a> agents learn by performing actions in the world and then analyzing the rewards they receive. Reinforcement learning differs from other machine learning approaches, e.g., supervised learning, because labeled input/output pairs, e.g., what actions lead to positive rewards are not presented to the agent <em>a priori</em>. Reinforcement learning agents learn what is good or bad by trying different actions.</p></li>
</ul>
</aside>
<hr class="docutils" />
<!-- 
Reinforcement learning agents must balance exploration of the environment, e.g., taking random actions with exploiting knowledge already obtained through interacting with the world. Pure exploration allows agents to construct a comprehensive model of their environment, but likely at the expense of positive reward. On the other hand, pure exploitation has the agent continually choosing the action it thinks best to accumulate reward, but different, better actions could be taken. 

A classic approach to understanding the exploration and exploitation tradeoff, which itself has many real-world applications, is the [multi arm bandit problem](https://en.wikipedia.org/wiki/Multi-armed_bandit). -->
<section id="bandit-problems">
<span id="content-references-multiarm-bandit-problems"></span><h2>Bandit problems<a class="headerlink" href="#bandit-problems" title="Link to this heading">#</a></h2>
<p>The <a class="reference external" href="https://en.wikipedia.org/wiki/Multi-armed_bandit">multi-armed bandit problem</a> is a fundamental problem in machine learning and decision-making. It refers to a scenario where an agent must repeatedly choose between multiple actions, each with an unknown reward distribution. Each action can be considered a slot machine, or a <em>one-armed bandit</em>, with potentially different payout distributions.</p>
<p>The goal of the agent is to maximize its total reward over time while learning the reward distributions of the different actions. This presents a tradeoff between exploitation, where the agent chooses the action with the highest estimated reward so far, and exploration, where it selects an action with an uncertain reward to learn more about its distribution. The multi-armed bandit problem has practical applications in various fields, such as clinical trials, online advertising, and recommender systems.</p>
<section id="bandit-problem-formulation">
<h3>Bandit problem formulation<a class="headerlink" href="#bandit-problem-formulation" title="Link to this heading">#</a></h3>
<p>A general bandit problem is a sequential game played between an agent and the environment. The game is played over <span class="math notranslate nohighlight">\(n\)</span> rounds called the <em>horizon</em>. In each round the agent chooses an action <span class="math notranslate nohighlight">\(a\in\mathcal{A}\)</span> and the environment then reveals a reward <span class="math notranslate nohighlight">\(r\)</span>. Let’s look at a particular sub-type of multi-arm bandit problem, namely, the Bernoulli bandit problem and Thompson sampling, a particular solution approach to this class of problem <span id="id1">[<a class="reference internal" href="../References.html#id5" title="Daniel Russo, Benjamin Van Roy, Abbas Kazerouni, Ian Osband, and Zheng Wen. A tutorial on thompson sampling. Foundations and Trends in Machine Learning, 11(1):1-96, 2017. URL: https://arxiv.org/abs/1707.02038, doi:10.48550/ARXIV.1707.02038.">Russo <em>et al.</em>, 2017</a>]</span>:</p>
<div class="proof definition admonition" id="defn-multi-arm-bernoulli-bandit-problem">
<p class="admonition-title"><span class="caption-number">Definition 51 </span> (Binary Bernoulli bandit problem)</p>
<section class="definition-content" id="proof-content">
<p>A Binary Bernoulli bandit problem is a type of multi-armed bandit problem where each action has a binary reward distribution with an unknown probability of success. In this problem:</p>
<ul class="simple">
<li><p>An agent has a choice between <span class="math notranslate nohighlight">\(K\)</span> possible actions <span class="math notranslate nohighlight">\(\mathcal{A}=\left\{a_{1},a_{2},\dots,a_{K}\right\}\)</span> where the success or failure of any action <span class="math notranslate nohighlight">\(a_{i}\in\mathcal{A}\)</span> is governed by a Bernoulli random variable (see <code class="xref prf prf-ref docutils literal notranslate"><span class="pre">defn-pmf-bernouli-random-variable</span></code>).</p></li>
<li><p>The success probabilities of any action <span class="math notranslate nohighlight">\(p_{a_{i}}\)</span>, while static over the horizon of the game, are unknown to the agent, but they can be learned by experimentation.</p></li>
<li><p>The agent receives a reward of <span class="math notranslate nohighlight">\(R_{a_{i}} = 1\)</span> (success) with probability <span class="math notranslate nohighlight">\(p_{a_{i}}\)</span>, and <span class="math notranslate nohighlight">\(R_{a_{i}} = 0\)</span> (failure) otherwise.</p></li>
</ul>
<p>The agent’s objective is to maximize the cumulative number of successes over the game horizon, e.g., T-periods, where <span class="math notranslate nohighlight">\(T\gg{K}\)</span>.</p>
</section>
</div><p>There are multiple heuristic strategies to solve the Binary Bernoulli bandit problem described in <a class="reference internal" href="#defn-multi-arm-bernoulli-bandit-problem">Definition 51</a>:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Thompson_sampling">Thompson sampling</a> which is shown in <a class="reference internal" href="#algo-thompson-sampling">Algorithm 14</a> consists of choosing the action that maximizes the expected reward with respect to a randomly drawn belief, where the belief distribution follows a <a class="reference external" href="https://en.wikipedia.org/wiki/Beta_distribution">Beta distribution</a>.</p></li>
</ul>
<div class="proof algorithm dropdown admonition" id="algo-thompson-sampling">
<p class="admonition-title"><span class="caption-number">Algorithm 14 </span> (Beta-Thompson-sampling)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Inputs</strong>  number of time periods in the horizon <span class="math notranslate nohighlight">\(T\)</span>, the dimension of the action space <span class="math notranslate nohighlight">\(K = |\mathcal{A}|\)</span>, and
<span class="math notranslate nohighlight">\(\alpha_{1},\dots,\alpha_{K}\)</span> and <span class="math notranslate nohighlight">\(\beta_{1},\dots,\beta_{K}\)</span> the initial values (prior) for the parameters that appear in the
Beta-distribution for each action</p>
<p><strong>Outputs</strong> posterior values for the parameters <span class="math notranslate nohighlight">\(\alpha_{1},\dots,\alpha_{K}\)</span> and <span class="math notranslate nohighlight">\(\beta_{1},\dots,\beta_{K}\)</span> that appear in the Beta-distributions for each action</p>
<p><strong>Initialize</strong>
a Beta-distribution for each action using initial values <span class="math notranslate nohighlight">\((\alpha,\beta)\)</span>.</p>
<p><strong>Main</strong></p>
<ol class="arabic simple">
<li><p>for t <span class="math notranslate nohighlight">\(\in\)</span> 1 to T</p>
<ol class="arabic simple">
<li><p>for k <span class="math notranslate nohighlight">\(\in\)</span> 1 to K</p>
<ol class="arabic simple">
<li><p>sample <span class="math notranslate nohighlight">\(\hat{\theta}_{k} \sim \beta(\alpha_{k}, \beta_{k})\)</span></p></li>
</ol>
</li>
<li><p>Select action: <span class="math notranslate nohighlight">\(a_{t} = \text{arg}\max_{k}\hat{\theta}_{k}\)</span></p></li>
<li><p>Apply <span class="math notranslate nohighlight">\(a_{t}\)</span> and observe <span class="math notranslate nohighlight">\(r_{t}\)</span>.</p></li>
<li><p>Update distribution parameters:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\((\alpha_{t},\beta_{t})\leftarrow\left(\alpha_{t}+r_{t},\beta_{t}+(1-r_{t})\right)\)</span></p></li>
</ol>
</li>
</ol>
</li>
</ol>
<p><strong>Return</strong>
<span class="math notranslate nohighlight">\((\alpha_{1},\dots,\alpha_{K})\)</span> and <span class="math notranslate nohighlight">\((\beta_{1},\dots,\beta_{K})\)</span></p>
</section>
</div><ul class="simple">
<li><p>An obvious (but naive) alternative approach is to allocate a fixed fraction of the time steps in the time horizon, say <span class="math notranslate nohighlight">\(\epsilon\)</span>, to explore purely random actions. In contrast, in the remaining periods, only successful actions are chosen via <a class="reference external" href="https://en.wikipedia.org/wiki/Thompson_sampling">Thompson sampling</a> or another exploitation approach. This strategy, called <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy exploration, is shown in <a class="reference internal" href="#algo-e-greedy">Algorithm 15</a> with a <a class="reference external" href="https://en.wikipedia.org/wiki/Thompson_sampling">Thompson sampling</a> exploitation step:</p></li>
</ul>
<div class="proof algorithm dropdown admonition" id="algo-e-greedy">
<p class="admonition-title"><span class="caption-number">Algorithm 15 </span> (<span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy exploration)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Inputs</strong>  the pure exploration parameter <span class="math notranslate nohighlight">\(\epsilon\)</span>, the number of time periods in the horizon <span class="math notranslate nohighlight">\(T\)</span>, the dimension of the action space <span class="math notranslate nohighlight">\(K = |\mathcal{A}|\)</span>, and
<span class="math notranslate nohighlight">\(\alpha_{1},\dots,\alpha_{K}\)</span> and <span class="math notranslate nohighlight">\(\beta_{1},\dots,\beta_{K}\)</span> the initial values (prior) for the parameters that appear in the
Beta-distribution for each action</p>
<p><strong>Outputs</strong> posterior values for the parameters <span class="math notranslate nohighlight">\(\alpha_{1},\dots,\alpha_{K}\)</span> and <span class="math notranslate nohighlight">\(\beta_{1},\dots,\beta_{K}\)</span> that appear in the Beta-distributions for each action</p>
<p><strong>Initialize</strong>
A Beta-distribution for each action using initial values <span class="math notranslate nohighlight">\((\alpha,\beta)\)</span>.</p>
<p><strong>Main</strong></p>
<ol class="arabic simple">
<li><p>for t <span class="math notranslate nohighlight">\(\in\)</span> 1 to T</p>
<ol class="arabic simple">
<li><p>if rand() <span class="math notranslate nohighlight">\(&lt;\epsilon\)</span></p>
<ol class="arabic simple">
<li><p>Select random action: <span class="math notranslate nohighlight">\(a_{t}\sim\text{Uniform}(\mathcal{A})\)</span></p></li>
</ol>
</li>
<li><p>else</p>
<ol class="arabic simple">
<li><p>for k <span class="math notranslate nohighlight">\(\in\)</span> 1 to K</p>
<ol class="arabic simple">
<li><p>sample <span class="math notranslate nohighlight">\(\hat{\theta}_{k} \sim \beta(\alpha_{k}, \beta_{k})\)</span></p></li>
</ol>
</li>
<li><p>Select action: <span class="math notranslate nohighlight">\(a_{t} = \text{arg}\max_{k}\hat{\theta}_{k}\)</span></p></li>
</ol>
</li>
<li><p>Apply <span class="math notranslate nohighlight">\(a_{t}\)</span> and observe <span class="math notranslate nohighlight">\(r_{t}\)</span>.</p></li>
<li><p>Update distribution parameters:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\((\alpha_{t},\beta_{t})\leftarrow\left(\alpha_{t}+r_{t},\beta_{t}+(1-r_{t})\right)\)</span></p></li>
</ol>
</li>
</ol>
</li>
</ol>
<p><strong>Return</strong>
<span class="math notranslate nohighlight">\((\alpha_{1},\dots,\alpha_{K})\)</span> and <span class="math notranslate nohighlight">\((\beta_{1},\dots,\beta_{K})\)</span></p>
</section>
</div></section>
</section>
<section id="basic-concepts-of-reinforcement-learning">
<span id="content-references-basic-concepts-reinforcement-learning"></span><h2>Basic concepts of reinforcement learning<a class="headerlink" href="#basic-concepts-of-reinforcement-learning" title="Link to this heading">#</a></h2>
<p>In our discussion of <a class="reference internal" href="markov.html#content-references-markov-decision-procesess"><span class="std std-ref">Markov decision process (MDPs)</span></a>, we assumed that the transition and reward models were known precisely. However, these models may not be discovered in many actual problems. In these cases, the agent must learn to act through experience, e.g., by observing the outcomes of its actions. Then the agent chooses actions that maximize its long-term accumulation of reward.</p>
<p>Several challenges must be addressed in cases of uncertain models. First, agents must balance between exploring the world and exploiting knowledge gained through experience. Second, rewards may be received long after decisions have been made. Finally, agents must generalize from limited experience.</p>
<figure class="align-default" id="fig-rl-schematic">
<a class="reference internal image-reference" href="../_images/Fig-Schematic-RL.svg"><img alt="../_images/Fig-Schematic-RL.svg" height="260px" src="../_images/Fig-Schematic-RL.svg" /></a>
<figcaption>
<p><span class="caption-number">Fig. 19 </span><span class="caption-text">Schematic of the reinforcement learning (RL) process. An agent takes action <span class="math notranslate nohighlight">\(a\)</span> and then observes reward <span class="math notranslate nohighlight">\(r\)</span> and changes in the state of the environment (<span class="math notranslate nohighlight">\(s\rightarrow{s^{\prime}}\)</span>) following the action <span class="math notranslate nohighlight">\(a\)</span>.</span><a class="headerlink" href="#fig-rl-schematic" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Reinforcement_learning">Reinforcement Learning (RL)</a> agents learn by performing actions in the world and then analyzing the rewards they receive (<a class="reference internal" href="#fig-rl-schematic"><span class="std std-numref">Fig. 19</span></a>). Thus, reinforcement learning differs from other machine learning approaches, e.g., <a class="reference external" href="https://en.wikipedia.org/wiki/Supervised_learning">supervised learning</a> because labeled input/output pairs, e.g., what actions lead to positive rewards are not presented to the agent <em>a priori</em>.  Instead, reinforcement learning agents learn what is good or bad by trying different actions.</p>
<p>Reinforcement learning agents learn optimal choices in different situations by balancing the exploration of their environment, e.g., by taking random actions and watching what happens, with the exploitation of the knowledge they have built up so far, i.e., choosing what the agent thinks is an optimal action based upon previous experience. The balance between exploration and exploitation is one of the critical concepts in reinforcement learning.</p>
<section id="model-free-reinforcement-learning">
<span id="content-references-model-free-reinforcement-learning"></span><h3>Model-free reinforcement learning<a class="headerlink" href="#model-free-reinforcement-learning" title="Link to this heading">#</a></h3>
<p>Model-free reinforcement learning does not require transition or reward models. Instead, model-free methods, like bandit problems, iteratively construct a policy by interacting with the world. However, unlike bandit problems, model-free reinforcement learning builds the action value function <span class="math notranslate nohighlight">\(Q(s, a)\)</span> directly, e.g., by incrementally estimating the action value function <span class="math notranslate nohighlight">\(Q(s, a)\)</span> from samples using the <a class="reference external" href="https://en.wikipedia.org/wiki/Q-learning">Q-learning approach</a>.</p>
<p>Model-free methods incrementally estimate the action value function <span class="math notranslate nohighlight">\(Q(s, a)\)</span> by sampling the world. To understand the structure of the update procedures, which we’ll discuss later, let’s take a quick detour and look at how we incrementally estimate the mean of a single variable <span class="math notranslate nohighlight">\(X\)</span>. Suppose we are interested in computing the mean of <span class="math notranslate nohighlight">\(X\)</span> from <span class="math notranslate nohighlight">\(m\)</span> samples:</p>
<div class="math notranslate nohighlight" id="equation-eqn-simple-mean">
<span class="eqno">(85)<a class="headerlink" href="#equation-eqn-simple-mean" title="Link to this equation">#</a></span>\[\hat{x}_{m} = \frac{1}{m}\sum_{i=1}^{m}x^{(i)}\]</div>
<p>where <span class="math notranslate nohighlight">\(x^{(i)}\)</span> denotes the ith sample. However, model-free RL incrementally updates <span class="math notranslate nohighlight">\(Q(s,a)\)</span>; thus, we need to develop an incremental estimation calculation.
Equation <a class="reference internal" href="#equation-eqn-simple-mean">(85)</a> can be re-written as:</p>
<div class="math notranslate nohighlight" id="equation-eqn-simple-mean-1">
<span class="eqno">(86)<a class="headerlink" href="#equation-eqn-simple-mean-1" title="Link to this equation">#</a></span>\[\hat{x}_{m} = \frac{1}{m}\left(x^{(m)}+\sum_{i=1}^{m-1}x^{(i)}\right)\]</div>
<p>but the summation term is mean from <span class="math notranslate nohighlight">\(m-1\)</span> samples or:</p>
<div class="math notranslate nohighlight" id="equation-eqn-simple-mean-2">
<span class="eqno">(87)<a class="headerlink" href="#equation-eqn-simple-mean-2" title="Link to this equation">#</a></span>\[\hat{x}_{m} = \frac{1}{m}\left(x^{(m)}+(m-1)\hat{x}_{m-1}\right)\]</div>
<p>which can be rearranged to give the incremental update expression:</p>
<div class="math notranslate nohighlight" id="equation-eqn-simple-mean-3">
<span class="eqno">(88)<a class="headerlink" href="#equation-eqn-simple-mean-3" title="Link to this equation">#</a></span>\[\hat{x}_{m} = \hat{x}_{m-1} + \frac{1}{m}\left(x^{(m)}-\hat{x}_{m-1}\right)\]</div>
<p>Equation <a class="reference internal" href="#equation-eqn-simple-mean-3">(88)</a> can be generalized to give the incremental update rule (<a class="reference internal" href="#defn-incremental-update-rule">Definition 52</a>):</p>
<div class="proof definition admonition" id="defn-incremental-update-rule">
<p class="admonition-title"><span class="caption-number">Definition 52 </span> (Incremental update rule)</p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(\hat{x}_{m-1}\)</span> denote the mean value computed from <span class="math notranslate nohighlight">\(m-1\)</span> samples. The value of <span class="math notranslate nohighlight">\(\hat{x}_{m}\)</span> given the the next sample <span class="math notranslate nohighlight">\(x^{(m)}\)</span> can be written as:</p>
<div class="math notranslate nohighlight" id="equation-eqn-next-sample-mean">
<span class="eqno">(89)<a class="headerlink" href="#equation-eqn-next-sample-mean" title="Link to this equation">#</a></span>\[\hat{x}_{m} = \hat{x}_{m-1} + \alpha\left(m\right)\left(x^{(m)}-\hat{x}_{m-1}\right)\]</div>
<p>where <span class="math notranslate nohighlight">\(\alpha\left(m\right)\)</span> is the learning rate function. The learning rate can be any function of <span class="math notranslate nohighlight">\(m\)</span>; however, to ensure convergence
<span class="math notranslate nohighlight">\(\alpha\left(m\right)\)</span> must have the properties: the sum of <span class="math notranslate nohighlight">\(\alpha\left(m\right)\)</span> as <span class="math notranslate nohighlight">\(m\rightarrow\infty\)</span> can be unbounded, but the sum of <span class="math notranslate nohighlight">\(\alpha\left(m\right)^{2}\)</span> as <span class="math notranslate nohighlight">\(m\rightarrow\infty\)</span> is bounded.</p>
</section>
</div><section id="q-learning">
<h4>Q-learning<a class="headerlink" href="#q-learning" title="Link to this heading">#</a></h4>
<p>The Q-learning approach incrementally estimates the action value function <span class="math notranslate nohighlight">\(Q(s,a)\)</span> using the action value form of the <em>Bellman expectation equation</em>.  From our discussion of <a class="reference internal" href="markov.html#content-references-markov-decision-procesess"><span class="std std-ref">Markov decision process (MDPs)</span></a>, we know that the action-value function (the <span class="math notranslate nohighlight">\(Q\)</span>-function) is defined as:</p>
<div class="math notranslate nohighlight">
\[Q(s,a) = R(s,a) + \gamma\sum_{s^{\prime}\in\mathcal{S}}T(s^{\prime} | s, a)U(s^{\prime})\]</div>
<p>However, we know that:</p>
<div class="math notranslate nohighlight">
\[U(s) = \max_{a} Q(s,a)\]</div>
<p>thus:</p>
<div class="math notranslate nohighlight" id="equation-eqn-bellman-expectation-eqn">
<span class="eqno">(90)<a class="headerlink" href="#equation-eqn-bellman-expectation-eqn" title="Link to this equation">#</a></span>\[Q(s,a) = R(s,a) + \gamma\sum_{s^{\prime}\in\mathcal{S}}T(s^{\prime} | s, a)\left(\max_{a^{\prime}}Q(s^{\prime},a^{\prime})\right)\]</div>
<p>Here’s the catch: in a model-free universe, we don’t know the rewards or physics of the world, i.e., we don’t know the rewards <span class="math notranslate nohighlight">\(R(s, a)\)</span> or the probability array <span class="math notranslate nohighlight">\(T(s^{\prime} | s, a)\)</span> that appear in Eqn. <a class="reference internal" href="#equation-eqn-bellman-expectation-eqn">(90)</a>. Instead of computing the rewards <span class="math notranslate nohighlight">\(r\)</span> and transitions from a model, we must estimate them from samples received:</p>
<div class="math notranslate nohighlight">
\[Q(s,a) = \mathbb{E}_{r,s^{\prime}}\left[r+\gamma\cdot\max_{a^{\prime}}Q(s^{\prime},a^{\prime})\right]\]</div>
<p>We can use the incremental update rule shown in Eqn. <a class="reference internal" href="#equation-eqn-next-sample-mean">(89)</a> to develop an an expression that incrementally updates our estimate of <span class="math notranslate nohighlight">\(Q(s,a)\)</span> as more data becomes available:</p>
<div class="math notranslate nohighlight" id="equation-eqn-q-learning-update-rule">
<span class="eqno">(91)<a class="headerlink" href="#equation-eqn-q-learning-update-rule" title="Link to this equation">#</a></span>\[Q(s,a)\leftarrow{Q(s,a)}+\alpha\left(r+\gamma\max_{a^{\prime}}Q(s^{\prime},a^{\prime}) - Q(s,a)\right)\]</div>
<p>where <span class="math notranslate nohighlight">\(\alpha\)</span> denotes a <em>learning rate hyperparameter</em>. Putting all these ideas together, gives a Q-learning definition (<a class="reference internal" href="#defn-q-learning-defn">Definition 53</a>):</p>
<div class="proof definition admonition" id="defn-q-learning-defn">
<p class="admonition-title"><span class="caption-number">Definition 53 </span> (Q-learning)</p>
<section class="definition-content" id="proof-content">
<p>There exists states <span class="math notranslate nohighlight">\(s\in\mathcal{S}\)</span> and actions <span class="math notranslate nohighlight">\(a\in\mathcal{A}\)</span>. The action value function <span class="math notranslate nohighlight">\(Q(s,a)\)</span> can be iteratively estimated using the update rule:</p>
<div class="math notranslate nohighlight">
\[Q(s,a)\leftarrow{Q(s,a)}+\alpha\left(r+\gamma\max_{a^{\prime}}Q(s^{\prime},a^{\prime}) - Q(s,a)\right)\]</div>
<p>where <span class="math notranslate nohighlight">\(\alpha\)</span> denotes the learning rate hyperparameter, and <span class="math notranslate nohighlight">\(\gamma\)</span> denotes the discount rate. The policy <span class="math notranslate nohighlight">\(\pi(s)\)</span> can be estimated from the action value function:</p>
<div class="math notranslate nohighlight">
\[\pi(s) = \text{arg}\max_{a}Q(s,a)\]</div>
</section>
</div><p>An algorithm to incrementally update the <span class="math notranslate nohighlight">\(Q\)</span>-function is given by (<a class="reference internal" href="#algo-e-greedy-q-learning">Algorithm 16</a>):</p>
<div class="proof algorithm dropdown admonition" id="algo-e-greedy-q-learning">
<p class="admonition-title"><span class="caption-number">Algorithm 16 </span> (<span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy Q-learning)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Inputs</strong>  the pure exploration parameter <span class="math notranslate nohighlight">\(\epsilon\)</span>, the number of time periods in the horizon <span class="math notranslate nohighlight">\(T\)</span>, the state space <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>, the action space <span class="math notranslate nohighlight">\(\mathcal{A}\)</span> and the initial state <span class="math notranslate nohighlight">\(s\)</span>.</p>
<p><strong>Outputs</strong> the <span class="math notranslate nohighlight">\(Q(s,a)\)</span> array</p>
<p><strong>Initialize</strong></p>
<ol class="arabic simple">
<li><p>set <span class="math notranslate nohighlight">\(Q(s,a)\leftarrow\text{zeros}(|\mathcal{S}|,|\mathcal{A}|)\)</span></p></li>
</ol>
<p><strong>Main</strong></p>
<ol class="arabic simple">
<li><p>for t <span class="math notranslate nohighlight">\(\in\)</span> 1 to T</p>
<ol class="arabic simple">
<li><p>if rand() <span class="math notranslate nohighlight">\(&lt;\epsilon\)</span></p>
<ol class="arabic simple">
<li><p>Select random action: <span class="math notranslate nohighlight">\(a_{t}\sim\text{Uniform}(\mathcal{A})\)</span></p></li>
</ol>
</li>
<li><p>else</p>
<ol class="arabic simple">
<li><p>Select action: <span class="math notranslate nohighlight">\(a_{t}\leftarrow\text{arg}\max_{a}Q(s,a)\)</span></p></li>
</ol>
</li>
<li><p>Apply <span class="math notranslate nohighlight">\(a_{t}\)</span>: observe the reward <span class="math notranslate nohighlight">\(r_{t}\)</span> and the state <span class="math notranslate nohighlight">\(s^{\prime}\)</span></p></li>
<li><p>Update <span class="math notranslate nohighlight">\(Q(s,a)\)</span>:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(Q(s,a)\leftarrow{Q(s,a)}+\alpha\left(r+\gamma\max_{a^{\prime}}Q(s^{\prime},a^{\prime}) - Q(s,a)\right)\)</span></p></li>
</ol>
</li>
<li><p>Update state:</p>
<ol class="arabic simple">
<li><p>set <span class="math notranslate nohighlight">\(s\leftarrow{s}^{\prime}\)</span></p></li>
</ol>
</li>
</ol>
</li>
</ol>
<p><strong>Return</strong>
<span class="math notranslate nohighlight">\(Q(s,a)\)</span></p>
</section>
</div></section>
</section>
</section>
</section>
<hr class="docutils" />
<section class="tex2jax_ignore mathjax_ignore" id="summary">
<h1>Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h1>
<p>In this lecture series, we explored bandit problems, and covered different algorithms and strategies for balancing exploration and exploitation:</p>
<ul class="simple">
<li><p><a class="reference internal" href="#content-references-multiarm-bandit-problems"><span class="std std-ref">Bandit problems</span></a> refer to a scenario where an agent must repeatedly choose between multiple actions, each with an unknown reward distribution. The goal of the agent is to maximize its total reward over time while learning the reward distributions of the different actions. This presents a tradeoff between exploration, where the agent chooses the action with the highest estimated reward so far, and exploitation, where it selects an action with an uncertain reward to learn more about its distribution.</p></li>
<li><p><a class="reference internal" href="#content-references-basic-concepts-reinforcement-learning"><span class="std std-ref">Reinforcement Learning (RL)</span></a> agents learn by performing actions in the world and then analyzing the rewards they receive. Reinforcement learning differs from other machine learning approaches, e.g., supervised learning, because labeled input/output pairs, e.g., what actions lead to positive rewards are not presented to the agent <em>a priori</em>. Reinforcement learning agents learn what is good or bad by trying different actions.</p></li>
</ul>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./unit-4-random"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="markov.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Markov Chains and Markov Decision Processes</p>
      </div>
    </a>
    <a class="right-next"
       href="../References.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">References</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Bandit Problems and Reinforcement Learning</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bandit-problems">Bandit problems</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bandit-problem-formulation">Bandit problem formulation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-concepts-of-reinforcement-learning">Basic concepts of reinforcement learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-free-reinforcement-learning">Model-free reinforcement learning</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#q-learning">Q-learning</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Varner
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>