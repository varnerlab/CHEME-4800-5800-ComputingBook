
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Ordinary Least Squares Problems &#8212; CHEME 4800/5800</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css?v=ca93fcec" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="../_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=36754332"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-VQRVBL1C02"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-VQRVBL1C02');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-VQRVBL1C02');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'unit-3-learning/penalty';</script>
    <link rel="canonical" href="https://varnerlab.github.io/CHEME-1800-Computing-Book/landing.html/unit-3-learning/penalty.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Linear Programming" href="lp.html" />
    <link rel="prev" title="The Learning Problem: Models, Learning, and Optimization" href="learning-landing.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../landing.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/cornell_seal_simple_black-164.svg" class="logo__image only-light" alt="CHEME 4800/5800 - Home"/>
    <script>document.write(`<img src="../_static/cornell_seal_simple_black-164.svg" class="logo__image only-dark" alt="CHEME 4800/5800 - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../landing.html">
                    Principles of Computational Thinking for Engineers
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../unit-1-basics/basics-landing.html">Unit 1. Computing Basics</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../unit-1-basics/types.html">Expressions, Variables, and Types</a></li>

<li class="toctree-l2"><a class="reference internal" href="../unit-1-basics/functions.html">Functions, Control Statements, Recursion and Testing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../unit-1-basics/programs.html">Programs and Modules</a></li>
<li class="toctree-l2"><a class="reference internal" href="../unit-1-basics/data-file-io.html">Data Input and Output</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../unit-2-data/data-landing.html">Unit 2. Data Structures</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../unit-2-data/trees.html">Data Structures</a></li>
<li class="toctree-l2"><a class="reference internal" href="../unit-2-data/vectors-matricies-nla.html">Vectors, Matrices and Linear Algebraic Equations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../unit-2-data/reduction.html">Dimensionality Reduction, Eigenvalues, and Eigenvectors</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="learning-landing.html">Unit 3. Learning and Optimization</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Ordinary Least Squares Problems</a></li>
<li class="toctree-l2"><a class="reference internal" href="lp.html">Linear Programming</a></li>
<li class="toctree-l2"><a class="reference internal" href="combitorial.html">Dynamic programming and Heuristic Optimization</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../References.html">References</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/varnerlab/CHEME-1800-Computing-Book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/varnerlab/CHEME-1800-Computing-Book/issues/new?title=Issue%20on%20page%20%2Funit-3-learning/penalty.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/unit-3-learning/penalty.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Ordinary Least Squares Problems</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression-models">Linear regression models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sum-of-squared-errors-loss-function">Sum of squared errors loss function</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#unconstrained-regression-problems">Unconstrained regression problems</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#square-regression-problems">Square regression problems</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overdetermined-regression-problems">Overdetermined regression problems</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#computation-of-the-overdetermined-matrix-inverse">Computation of the overdetermined matrix inverse</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#underdetermined-regression-problems">Underdetermined regression problems</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#least-norm-solutions">Least-norm solutions</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#computation-of-the-underdetermined-matrix-inverse">Computation of the underdetermined matrix inverse</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#constrained-regression-problems">Constrained regression problems</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lagrange-multipliers">Lagrange multipliers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#penalty-methods">Penalty methods</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#quadratic-penalty-functions">Quadratic penalty functions</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#barrier-functions">Barrier functions</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#application-of-penalty-and-barrier-methods">Application of penalty and barrier methods</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="ordinary-least-squares-problems">
<h1>Ordinary Least Squares Problems<a class="headerlink" href="#ordinary-least-squares-problems" title="Link to this heading">#</a></h1>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Link to this heading">#</a></h2>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Ordinary_least_squares">Ordinary least squares</a> is a statistical method to estimate the parameters of a regression model, i.e., to find the best fit model that predicts the values of the response variable (dependent variable) based on the values of the observed variables (independent variables).</p>
<p>We start the lecture by introducing the linear regression models, and the learning problem, and then explore two types of common ordinary least squares problems:</p>
<ul class="simple">
<li><p><a class="reference internal" href="#content-references-ols-general-introduction"><span class="std std-ref">Linear regression models</span></a> is a statistical model used to analyze the relationship between a dependent variable (also known as the response variable) and one or more independent variables (also known as explanatory variables or predictors) that are believed to influence the dependent variable.</p></li>
<li><p><a class="reference internal" href="#content-references-ols-unconstrained-problem"><span class="std std-ref">Unconstrained regression problems</span></a>. In unconstrained problems, the regression model parameters can take on any values and are not limited in any way.</p></li>
<li><p><a class="reference internal" href="#content-references-ols-constrained-problem"><span class="std std-ref">Constrained regression problems</span></a>. In constrained problems, the regression model parameters can be bound in some way, e.g., by physical reasoning or by some relationships between the parameters.</p></li>
</ul>
<hr class="docutils" />
<!-- is a popular method for estimating the parameters of regression models because it is relatively simple to implement and has nice mathematical properties, such as being unbiased and having the smallest variance among all unbiased estimators. -->
</section>
<section id="linear-regression-models">
<span id="content-references-ols-general-introduction"></span><h2>Linear regression models<a class="headerlink" href="#linear-regression-models" title="Link to this heading">#</a></h2>
<p>Linear regression models are a statistical model used to analyze the relationship between a dependent variable (also known as the response variable) and one or more independent variables (also known as explanatory variables or predictors) that are believed to influence the dependent variable (<a class="reference internal" href="#defn-ols-introduction">Definition 34</a>):</p>
<div class="proof definition admonition" id="defn-ols-introduction">
<p class="admonition-title"><span class="caption-number">Definition 34 </span> (Linear regression model)</p>
<section class="definition-content" id="proof-content">
<p>There exists a dataset <span class="math notranslate nohighlight">\(\mathcal{D} = \left\{\mathbf{x}_{i},y_{i}\right\}_{i=1}^{n}\)</span> where <span class="math notranslate nohighlight">\(\mathbf{x}_{i}\)</span> is a p-vector of inputs (independent variables) and <span class="math notranslate nohighlight">\(y_{i}\)</span> denotes a scalar response variable (dependent variable). Then, a linear regression model for <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> takes the form:</p>
<div class="math notranslate nohighlight" id="equation-eqn-linear-regression-model">
<span class="eqno">(62)<a class="headerlink" href="#equation-eqn-linear-regression-model" title="Link to this equation">#</a></span>\[y_{i} = \mathbf{x}_{i}^{T}\mathbf{\beta} + \epsilon_{i}\qquad{i=1,2,\dots,n}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{x}_{i}\)</span> is a p-dimensional vector of inputs, <span class="math notranslate nohighlight">\(\mathbf{\beta}\)</span> is a <span class="math notranslate nohighlight">\(p\times{1}\)</span> vector of unknown model parameters and <span class="math notranslate nohighlight">\(\epsilon_{i}\)</span> represents unobserved random errors. Eqn. <a class="reference internal" href="#equation-eqn-linear-regression-model">(62)</a> can be re-written in matrix form as:</p>
<div class="math notranslate nohighlight" id="equation-eqn-linear-regression-model-matrix">
<span class="eqno">(63)<a class="headerlink" href="#equation-eqn-linear-regression-model-matrix" title="Link to this equation">#</a></span>\[\mathbf{y} = \mathbf{X}\mathbf{\beta} + \mathbf{\epsilon}\]</div>
</section>
</div><p>Linear reagression models are widely used to model physical, chemical or economic phenomena, e.g., a model of the rate of a chemical reaction, the return of a stock, the relationship between the home price and square footage, etc.</p>
<figure class="align-default" id="fig-linear-regression-schematic">
<a class="reference internal image-reference" href="unit-3-learning/figs/Fig-LinearRegression-Schematic.pdf"><img alt="unit-3-learning/figs/Fig-LinearRegression-Schematic.pdf" src="unit-3-learning/figs/Fig-LinearRegression-Schematic.pdf" style="height: 320px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 15 </span><span class="caption-text">Schematic of linear regression. Unknown model parameters <span class="math notranslate nohighlight">\(\mathbf{\beta}\)</span> are chosen so that the squared difference (residual) between model predicted <span class="math notranslate nohighlight">\(\hat{y}_{i}=\mathbf{x}_{i}^{T}\mathbf{\beta}\)</span> and observed <span class="math notranslate nohighlight">\(y_{i}\)</span> output variables is minimized.</span><a class="headerlink" href="#fig-linear-regression-schematic" title="Link to this image">#</a></p>
</figcaption>
</figure>
<section id="sum-of-squared-errors-loss-function">
<h3>Sum of squared errors loss function<a class="headerlink" href="#sum-of-squared-errors-loss-function" title="Link to this heading">#</a></h3>
<p>Starting from observations, <a class="reference external" href="https://en.wikipedia.org/wiki/Ordinary_least_squares">ordinary least squares</a> estimates the value of the unknown parameters <span class="math notranslate nohighlight">\(\mathbf{\beta}\)</span> that appear in a linear regression model by <em>minimizing</em> the sum of squared errors between model estimates and observed values as illustrated in <a class="reference internal" href="#fig-linear-regression-schematic"><span class="std std-numref">Fig. 15</span></a> and defined in (<a class="reference internal" href="#defn-sum-squared-error">Definition 35</a>):</p>
<div class="proof definition admonition" id="defn-sum-squared-error">
<p class="admonition-title"><span class="caption-number">Definition 35 </span> (Sum of squared error loss function)</p>
<section class="definition-content" id="proof-content">
<p>There exists a dataset <span class="math notranslate nohighlight">\(\mathcal{D} = \left\{\mathbf{x}_{i},y_{i}\right\}_{i=1}^{n}\)</span> where <span class="math notranslate nohighlight">\(\mathbf{x}_{i}\)</span> is a p-vector of observed inputs and <span class="math notranslate nohighlight">\(y_{i}\)</span> denotes an observed response variable. Further, suppose we model the dataset <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> using the linear regression model:</p>
<div class="math notranslate nohighlight">
\[\mathbf{y} = \mathbf{X}\mathbf{\beta} + \mathbf{\epsilon}\]</div>
<p>Best-fit values for the unknown parameters <span class="math notranslate nohighlight">\(\mathbf{\beta}\)</span> can be estimated by solving the optimization problem:</p>
<div class="math notranslate nohighlight" id="equation-eqn-obj-f-ols">
<span class="eqno">(64)<a class="headerlink" href="#equation-eqn-obj-f-ols" title="Link to this equation">#</a></span>\[\hat{\mathbf{\beta}} = \arg\min_{\mathbf{\beta}} ||~\mathbf{y} - \mathbf{X}\mathbf{\beta}~||^{2}_{2}\]</div>
<p>where <span class="math notranslate nohighlight">\(||\star||^{2}_{2}\)</span> is the square of the p = 2 vector norm, see <span class="xref std std-ref">content:measurements-matrix-vector-norms</span>.</p>
</section>
</div><p><a class="reference external" href="https://en.wikipedia.org/wiki/Ordinary_least_squares">Ordinary least squares</a> is an example of a <a class="reference external" href="https://en.wikipedia.org/wiki/Supervised_learning">supervised learning approach</a>, i.e., we use measured inputs and outputs to estimate parameters appearing in the model.</p>
<!-- the sum of the squared differences between the observed values of the response variable and the predicted values based on the regression line. This is done by choosing the values of the model parameters that minimize the sum of squared errors (SSE). However, OLS has some limitations, such as assuming that the errors in the model are Normally distributed and having poor performance when the data is highly correlated or has outliers. -->
</section>
</section>
<section id="unconstrained-regression-problems">
<span id="content-references-ols-unconstrained-problem"></span><h2>Unconstrained regression problems<a class="headerlink" href="#unconstrained-regression-problems" title="Link to this heading">#</a></h2>
<p>In an unconstrained least-squares problem, we find parameter values that minimize the difference between the model predictions and the observed data. Least-squares problems are <em>unconstrained</em> when there are no constraints on the permissible values of the parameters.</p>
<p>However, the data matrix <span class="math notranslate nohighlight">\(\mathbf{X}\in\mathbb{R}^{n\times{p}}\)</span> can have different shapes:</p>
<ul class="simple">
<li><p><strong>Square</strong>: the number of observations (rows) is the same as the number of parameters (columns), in which case the system is square (<span class="math notranslate nohighlight">\(n = p\)</span>).</p></li>
<li><p><strong>Overdetermined</strong>: the data matrix <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> has more observations (rows) than parameters (columns), in which case the system is overdetermined (<span class="math notranslate nohighlight">\(n\gg{p}\)</span>).</p></li>
<li><p><strong>Underdetermined</strong>: the data matrix <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> has fewer observations (rows) than parameters (columns), in which case the system is underdetermined (<span class="math notranslate nohighlight">\(n\ll{p}\)</span>).</p></li>
</ul>
<section id="square-regression-problems">
<h3>Square regression problems<a class="headerlink" href="#square-regression-problems" title="Link to this heading">#</a></h3>
<p>Suppose we are estimating the values of the unknown parameters <span class="math notranslate nohighlight">\(\mathbf{\beta}\)</span> in a linear regression model:</p>
<div class="math notranslate nohighlight" id="equation-eqn-square-ols-model">
<span class="eqno">(65)<a class="headerlink" href="#equation-eqn-square-ols-model" title="Link to this equation">#</a></span>\[\mathbf{y} = \mathbf{X}\mathbf{\beta} + \mathbf{\epsilon}\]</div>
<p>where the data matrix <span class="math notranslate nohighlight">\(\mathbf{X}\in\mathbb{R}^{p\times{p}}\)</span> has the same number of rows and columns as the number of unknown parameters. In this case, Eqn <a class="reference internal" href="#equation-eqn-square-ols-model">(65)</a> is a square system of linear algebraic equations which we can solve for the unknown parameter vector <span class="math notranslate nohighlight">\(\mathbf{\beta}\)</span> by solving the system of equations:</p>
<div class="math notranslate nohighlight">
\[\hat{\mathbf{\beta}} = \mathbf{X}^{-1}\mathbf{y} - \mathbf{X}^{-1}\mathbf{\epsilon}\]</div>
<p>either directly or iteratively, see our previous discussion of <a class="reference internal" href="../unit-2-data/vectors-matricies-nla.html#content-references-soln-laes-start"><span class="std std-ref">Linear algebraic equations</span></a>.
However, for this approach to be applicable, the inverse of the data matrix must exist, see <a class="reference internal" href="../unit-2-data/vectors-matricies-nla.html#defn-matrix-inverse">Definition 23</a>.</p>
</section>
<section id="overdetermined-regression-problems">
<h3>Overdetermined regression problems<a class="headerlink" href="#overdetermined-regression-problems" title="Link to this heading">#</a></h3>
<p>In most applications, it is more likely that the data matrix <span class="math notranslate nohighlight">\(\mathbf{X}\in\mathbb{R}^{n\times{p}}\)</span> will be overdetermined (<span class="math notranslate nohighlight">\(n\gg{p}\)</span>). The inverse of an overdetermined data matrix can not be computed directly. Instead, we solve a particular system of equations called the <a class="reference external" href="https://en.wikipedia.org/wiki/Ordinary_least_squares">normal equations</a> which transforms the original overdetermined problem into a square system:</p>
<div class="proof definition admonition" id="defn-normal-eqn-ols">
<p class="admonition-title"><span class="caption-number">Definition 36 </span> (Normal solution overdetermined linear regression model)</p>
<section class="definition-content" id="proof-content">
<p>There exists dataset <span class="math notranslate nohighlight">\(\mathcal{D} = \left\{\mathbf{x}_{i},y_{i}\right\}_{i=1}^{n}\)</span> where <span class="math notranslate nohighlight">\(\mathbf{x}_{i}\)</span> is a p-dimensional vector of inputs (independent variables) and <span class="math notranslate nohighlight">\(y_{i}\)</span> denotes a scalar response variable (dependent variable), and <span class="math notranslate nohighlight">\(n\gg{p}\)</span>.  Further, suppose we model the dataset <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> using the linear regression model:</p>
<div class="math notranslate nohighlight">
\[\mathbf{y} = \mathbf{X}\mathbf{\beta} + \mathbf{\epsilon}\]</div>
<p>Then, the value of the unknown parameter vector <span class="math notranslate nohighlight">\(\mathbf{\beta}\)</span> that minimizes the sum of the squares loss function for an overdetermined system is given by:</p>
<div class="math notranslate nohighlight" id="equation-eqn-loss-function-soln">
<span class="eqno">(66)<a class="headerlink" href="#equation-eqn-loss-function-soln" title="Link to this equation">#</a></span>\[\hat{\mathbf{\beta}} = \left(\mathbf{X}^{T}\mathbf{X}\right)^{-1}\mathbf{X}^{T}\mathbf{y} - \left(\mathbf{X}^{T}\mathbf{X}\right)^{-1}\mathbf{X}^{T}\mathbf{\epsilon}\]</div>
<p>The matrix <span class="math notranslate nohighlight">\(\mathbf{X}^{T}\mathbf{X}\)</span> is called the normal matrix, while <span class="math notranslate nohighlight">\(\mathbf{X}^{T}\mathbf{y}\)</span> is called the moment matrix. The existence of the normal solution <span class="math notranslate nohighlight">\(\hat{\mathbf{\beta}}\)</span> requires that the normal matrix inverse <span class="math notranslate nohighlight">\(\left(\mathbf{X}^{T}\mathbf{X}\right)^{-1}\)</span> exists.</p>
</section>
</div><section id="computation-of-the-overdetermined-matrix-inverse">
<h4>Computation of the overdetermined matrix inverse<a class="headerlink" href="#computation-of-the-overdetermined-matrix-inverse" title="Link to this heading">#</a></h4>
<p>Assuming the normal matrix inverse <span class="math notranslate nohighlight">\(\left(\mathbf{X}^{T}\mathbf{X}\right)^{-1}\)</span> exists, we can compute it directly by computing the matrix product and inverting using the <code class="docutils literal notranslate"><span class="pre">inv</span></code> function. Alternatively, we can use the <a class="reference external" href="https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.pinv">pinv</a> function included in the <code class="docutils literal notranslate"><span class="pre">LinearAlgebra</span></code> package in <a class="reference external" href="https://julialang.org">Julia</a>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="c"># load the LinearAlgebra package</span>
<span class="k">using</span><span class="w"> </span><span class="n">LinearAlgebra</span>

<span class="c"># Define a random overdetermined array</span>
<span class="n">A</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">8</span><span class="p">)</span><span class="w"> </span>

<span class="c"># compute the pinverse </span>
<span class="n">LI</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">pinv</span><span class="p">(</span><span class="n">A</span><span class="p">);</span><span class="w"> </span><span class="c"># this will be an 8 x 10 matrix</span>

<span class="c"># Test -</span>
<span class="n">IM</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">LI</span><span class="o">*</span><span class="n">A</span><span class="p">;</span><span class="w"> </span>
<span class="n">println</span><span class="p">(</span><span class="s">&quot;Is IM the identity matrix -&gt; check the trace: </span><span class="si">$</span><span class="p">(</span><span class="n">tr</span><span class="p">(</span><span class="n">IM</span><span class="p">))</span><span class="s">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Is IM the identity matrix -&gt; check the trace: 8.0
</pre></div>
</div>
</div>
</div>
<p>This matrix inverse is called a <a class="reference external" href="https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse">left Moore-Penrose inverse</a>.</p>
</section>
</section>
<section id="underdetermined-regression-problems">
<h3>Underdetermined regression problems<a class="headerlink" href="#underdetermined-regression-problems" title="Link to this heading">#</a></h3>
<p>If the number of observations (rows) in the data matrix <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is less than the number of columns, i.e., the number of unknown parameters, the system is underdetermined. In general, there will be infinitely many solutions for an underdetermined system. How do we choose which solution to use?</p>
<section id="least-norm-solutions">
<h4>Least-norm solutions<a class="headerlink" href="#least-norm-solutions" title="Link to this heading">#</a></h4>
<p>The classic strategy to solve an underdetermined system is to estimate the <em>smallest</em> values (measured by some norm <span class="math notranslate nohighlight">\(||\star||\)</span>) for the unknown parameters <span class="math notranslate nohighlight">\(\mathbf{\beta}\)</span> such that they satisfy the original equations:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{eqnarray}
\text{minimize}~&amp; &amp; ||~\mathbf{\beta}~|| \\
\text{subject to} &amp; &amp; \mathbf{X}\mathbf{\beta} = \mathbf{y} \\
\end{eqnarray}\end{split}\]</div>
<p>Thus, the solution of an underdetermined least squares problem is <em>constrained</em>, i.e., the possible values of <span class="math notranslate nohighlight">\(\mathbf{\beta}\)</span> are restricted somehow. An analytical solution for the parmeter vector <span class="math notranslate nohighlight">\(\mathbf{\beta}\)</span> can be computed for the underdetermined case (<a class="reference internal" href="#defn-normal-eqn-ols-ln">Definition 37</a>):</p>
<div class="proof definition admonition" id="defn-normal-eqn-ols-ln">
<p class="admonition-title"><span class="caption-number">Definition 37 </span> (Solution underdetermined linear regression model)</p>
<section class="definition-content" id="proof-content">
<p>There exists dataset <span class="math notranslate nohighlight">\(\mathcal{D} = \left\{\mathbf{x}_{i},y_{i}\right\}_{i=1}^{n}\)</span> where <span class="math notranslate nohighlight">\(\mathbf{x}_{i}\)</span> is a p-dimensional vector of inputs (independent variables) and <span class="math notranslate nohighlight">\(y_{i}\)</span> denotes a scalar response variable (dependent variable), and <span class="math notranslate nohighlight">\(n\gg{p}\)</span>.  Further, we model the dataset <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> using the linear regression model:</p>
<div class="math notranslate nohighlight">
\[\mathbf{y} = \mathbf{X}\mathbf{\beta} + \mathbf{\epsilon}\]</div>
<p>Then, the least-norm solution of the unknown parameter vector <span class="math notranslate nohighlight">\(\mathbf{\beta}\)</span> is given by:</p>
<div class="math notranslate nohighlight" id="equation-eqn-loss-function-soln-least-norm">
<span class="eqno">(67)<a class="headerlink" href="#equation-eqn-loss-function-soln-least-norm" title="Link to this equation">#</a></span>\[\hat{\mathbf{\beta}} =\mathbf{X}^{T}\left(\mathbf{X}\mathbf{X}^{T}\right)^{-1}\mathbf{y} - \mathbf{X}^{T}\left(\mathbf{X}\mathbf{X}^{T}\right)^{-1}\mathbf{\epsilon}\]</div>
<p>The existence of the least-norm solution <span class="math notranslate nohighlight">\(\hat{\mathbf{\beta}}\)</span> requires that the matrix inverse <span class="math notranslate nohighlight">\(\left(\mathbf{X}\mathbf{X}^{T}\right)^{-1}\)</span> exists.</p>
</section>
</div><p>Equivalently, we can solve for the unknown model parameters <span class="math notranslate nohighlight">\(\mathbf{\beta}\)</span> using singular value decomposition (SVD) of the data matrix (<a class="reference internal" href="#defn-svd-soln-ud-ols">Definition 38</a>):</p>
<div class="proof definition admonition" id="defn-svd-soln-ud-ols">
<p class="admonition-title"><span class="caption-number">Definition 38 </span> (Singular value decomposition underdetermined system)</p>
<section class="definition-content" id="proof-content">
<p>There exists a dataset <span class="math notranslate nohighlight">\(\mathcal{D} = \left\{\mathbf{x}_{i},y_{i}\right\}_{i=1}^{n}\)</span> where <span class="math notranslate nohighlight">\(\mathbf{x}_{i}\)</span> is a p-vector of inputs (independent variables) and <span class="math notranslate nohighlight">\(y_{i}\)</span> denotes a scalar response variable (dependent variable), and <span class="math notranslate nohighlight">\(n\ll{p}\)</span>.  Further, suppose we model the dataset <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> using the linear regression model:</p>
<div class="math notranslate nohighlight">
\[\mathbf{y} = \mathbf{X}\mathbf{\beta} + \mathbf{\epsilon}\]</div>
<p>Then, the smallest values of the unknown parameter vector <span class="math notranslate nohighlight">\(\mathbf{\beta}\)</span> that satisfies the linear regression model are given by:</p>
<div class="math notranslate nohighlight">
\[\hat{\mathbf{\beta}} = \mathbf{V}\mathbf{\Sigma}^{-1}\mathbf{U}^{T}\mathbf{y} - \mathbf{V}\mathbf{\Sigma}^{-1}\mathbf{U}^{T}\mathbf{\epsilon}\]</div>
</section>
</div></section>
<section id="computation-of-the-underdetermined-matrix-inverse">
<h4>Computation of the underdetermined matrix inverse<a class="headerlink" href="#computation-of-the-underdetermined-matrix-inverse" title="Link to this heading">#</a></h4>
<p>We can compute the matrix inverse <span class="math notranslate nohighlight">\(\left(\mathbf{X}\mathbf{X}^{T}\right)^{-1}\)</span>, assuming it exists, by computing the matrix product and inverting using the <code class="docutils literal notranslate"><span class="pre">inv</span></code> function. Alternatively, we can use the <a class="reference external" href="https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.pinv">pinv</a> function included in the <code class="docutils literal notranslate"><span class="pre">LinearAlgebra</span></code> package in <a class="reference external" href="https://julialang.org">Julia</a>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="c"># load the LinearAlgebra package</span>
<span class="k">using</span><span class="w"> </span><span class="n">LinearAlgebra</span>

<span class="c"># Define a random undetermined array</span>
<span class="n">A</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rand</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span><span class="w"> </span>

<span class="c"># compute the pinverse </span>
<span class="n">RI</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">pinv</span><span class="p">(</span><span class="n">A</span><span class="p">);</span><span class="w"> </span><span class="c"># this will be an 10 x 8 matrix</span>

<span class="c"># Test -</span>
<span class="n">IM</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">A</span><span class="o">*</span><span class="n">RI</span><span class="p">;</span><span class="w"> </span>
<span class="n">println</span><span class="p">(</span><span class="s">&quot;Is IM the identity matrix -&gt; check the trace: </span><span class="si">$</span><span class="p">(</span><span class="n">tr</span><span class="p">(</span><span class="n">IM</span><span class="p">))</span><span class="s">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Is IM the identity matrix -&gt; check the trace: 8.0
</pre></div>
</div>
</div>
</div>
<p>This matrix inverse is called a <a class="reference external" href="https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse">right Moore-Penrose inverse</a>.</p>
<!-- This is done by defining a cost function that measures the difference between the predictions and the experimental data and then finding the parameter values that minimize this cost function. Least-squares problems are commonly used in linear regression, where the goal is to find the line of best fit for a set of data points. They are also used in many other applications, such as curve fitting, signal processing, and control systems.

Many optimization algorithms can solve least-squares problems, including gradient descent, Levenberg-Marquardt, and Gauss-Newton algorithms. The choice of algorithm will depend on the problem being solved and the characteristics of the data. -->
</section>
</section>
</section>
<section id="constrained-regression-problems">
<span id="content-references-ols-constrained-problem"></span><h2>Constrained regression problems<a class="headerlink" href="#constrained-regression-problems" title="Link to this heading">#</a></h2>
<p>Constrained least squares estimates the parameters of a linear regression model subject to one or more constraints on the values the parameters can take, e.g.,
there exists prior knowledge or physical relationships that must be satisfied by the parameter estimates.</p>
<!-- To solve constrained least squares problems, we first define the linear regression model and the constraints on the parameters. We then define a loss function that measures the model’s fit to the data subject to the constraints. We minimize the loss function to find the estimates of the parameters that best fit the data while satisfying the constraints. -->
<section id="lagrange-multipliers">
<h3>Lagrange multipliers<a class="headerlink" href="#lagrange-multipliers" title="Link to this heading">#</a></h3>
<p>Lagrange multipliers are a mathematical tool used in optimization to find a function’s maximum or minimum value subject to constraints. The <a class="reference external" href="https://en.wikipedia.org/wiki/Lagrange_multiplier">method of Lagrange multipliers</a> involves introducing additional variables, called Lagrange multipliers, which integrate the constraints into the loss function (<a class="reference internal" href="#defn-method-l-multipliers">Definition 39</a>):</p>
<div class="proof definition admonition" id="defn-method-l-multipliers">
<p class="admonition-title"><span class="caption-number">Definition 39 </span> (Method of Lagrange multipliers)</p>
<section class="definition-content" id="proof-content">
<p>To find the maximum or minimum of a function <span class="math notranslate nohighlight">\(f(x)\)</span> subject to the equality constraint <span class="math notranslate nohighlight">\(g(x)\)</span>, we can form the Lagrangian function:</p>
<div class="math notranslate nohighlight" id="equation-eqn-lagrangian-1d">
<span class="eqno">(68)<a class="headerlink" href="#equation-eqn-lagrangian-1d" title="Link to this equation">#</a></span>\[\mathcal{L}(x,\lambda) = f(x) + \lambda\cdot{g}(x)\]</div>
<p>where <span class="math notranslate nohighlight">\(\lambda\)</span> is the Lagrange multiplier for constraint <span class="math notranslate nohighlight">\(g(x)\)</span>, and <span class="math notranslate nohighlight">\(\mathcal{L}(x,\lambda)\)</span> is called the Lagrangian function. At a critical point (maximum or minimum), the partial derivatives of the
Lagrangian function with respect to <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(\lambda\)</span> vanish:</p>
<div class="math notranslate nohighlight" id="equation-eqn-first-order-condition-lagrange">
<span class="eqno">(69)<a class="headerlink" href="#equation-eqn-first-order-condition-lagrange" title="Link to this equation">#</a></span>\[\begin{split}\begin{eqnarray}
\frac{\partial\mathcal{L}}{\partial{x}} &amp; = &amp; 0\\
\frac{\partial\mathcal{L}}{\partial{\lambda}} &amp; = &amp; 0\\
\end{eqnarray}\end{split}\]</div>
<p>The system of equations defined by Eqn. <a class="reference internal" href="#equation-eqn-first-order-condition-lagrange">(69)</a> callled the Lagrange equations, can be solved to find the critical points and, thus, the maximum or minimum value of the objective function.</p>
</section>
</div><p>Let’s show how we can use the <a class="reference external" href="https://en.wikipedia.org/wiki/Lagrange_multiplier">method of Lagrange multipliers</a> to derive the least-norm solution for the underdetermined learning problem (<a class="reference internal" href="#example-lagrange-multipliers-lns">Example 21</a>):</p>
<div class="proof example dropdown admonition" id="example-lagrange-multipliers-lns">
<p class="admonition-title"><span class="caption-number">Example 21 </span> (Derivation least norm solution)</p>
<section class="example-content" id="proof-content">
<p>Derive the least norm solution to the optimization problem:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{eqnarray}
\text{minimize} &amp; &amp; \mathbf{\beta}^{T}\mathbf{\beta} \\
\text{subject to} &amp; &amp; \mathbf{X}\mathbf{\beta} = \mathbf{y}
\end{eqnarray}\end{split}\]</div>
<p>using the <a class="reference external" href="https://en.wikipedia.org/wiki/Lagrange_multiplier">method of Lagrange multipliers</a> to compute an estimate of the regression parameters <span class="math notranslate nohighlight">\(\mathbf{\beta}\)</span>.</p>
<p><strong>Solution</strong>: The first step is to form the Lagrangian function:</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(\mathbf{\beta},\lambda) = \mathbf{\beta}^{T}\mathbf{\beta} + \lambda\left(\mathbf{X}\mathbf{\beta} - \mathbf{y}\right)\]</div>
<p>which can differentiate with respect to <span class="math notranslate nohighlight">\(\mathbf{\beta}\)</span> and the Lagrange multipliers <span class="math notranslate nohighlight">\(\lambda\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{eqnarray}
\frac{\partial\mathcal{L}}{\partial{\mathbf{\beta}}} &amp; = &amp; 2\mathbf{\beta} + \mathbf{X}^{T}\lambda = 0 \\
\frac{\partial\mathcal{L}}{\partial{\lambda}} &amp; = &amp; \left(\mathbf{X}\mathbf{\beta} - \mathbf{y}\right) = 0\\
\end{eqnarray}\end{split}\]</div>
<p>We can solve the first equation for <span class="math notranslate nohighlight">\(\mathbf{\beta}\)</span> in terms of <span class="math notranslate nohighlight">\(\lambda\)</span>:</p>
<div class="math notranslate nohighlight">
\[\mathbf{\beta} = -\mathbf{X}^{T}\left(\frac{\lambda}{2}\right)\]</div>
<p>which we can then substitute into the second equation to get an expression for <span class="math notranslate nohighlight">\(\lambda = -2\left(\mathbf{X}\mathbf{X}^{T}\right)^{-1}\mathbf{y}\)</span>. Substituting the <span class="math notranslate nohighlight">\(\lambda\)</span> expression into the expression for <span class="math notranslate nohighlight">\(\beta\)</span>, i.e., eliminating the <a class="reference external" href="https://en.wikipedia.org/wiki/Lagrange_multiplier">Lagrange multipliers</a> gives the least norm solution for <span class="math notranslate nohighlight">\(\mathbf{\beta}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\mathbf{\beta} = \mathbf{X}^{T}\left(\mathbf{X}\mathbf{X}^{T}\right)^{-1}\mathbf{y}\]</div>
</section>
</div><!-- 
The Lagrangian, which is the sum of the objective function and the product of the Lagrange multipliers and the constraints, is used to find the critical points of the objective function subject to the constraints by taking the partial derivatives with respect to both the variables and the Lagrange multipliers and setting them to zero. -->
</section>
<section id="penalty-methods">
<h3>Penalty methods<a class="headerlink" href="#penalty-methods" title="Link to this heading">#</a></h3>
<p>A penalty method transforms a constrained least squares problem into an unconstrained problem that can be solved. In a penalty method, a penalty is added to the loss function to encourage specific desirable properties of the solution.</p>
<section id="quadratic-penalty-functions">
<h4>Quadratic penalty functions<a class="headerlink" href="#quadratic-penalty-functions" title="Link to this heading">#</a></h4>
<p>Before we look at the applications of penalty methods, let’s consider a simple example to work out the basic strategy. This example was reproduced from the <a class="reference external" href="https://web.stanford.edu/group/sisl/k12/optimization/#!index.md">Mathematical Optimization course at Stanford</a>. Suppose we wanted to solve the problem:</p>
<div class="math notranslate nohighlight" id="equation-eqn-example-pmethod">
<span class="eqno">(70)<a class="headerlink" href="#equation-eqn-example-pmethod" title="Link to this equation">#</a></span>\[\begin{split}\begin{eqnarray}
\arg \min_{x} \left(f(x) = \frac{100}{x}\right) &amp; &amp;  \\
\text{subject to} &amp; &amp; \\
x\leq{5} &amp; &amp;   
\end{eqnarray}\end{split}\]</div>
<p>Before starting, convert any constraints into the form (expression) <span class="math notranslate nohighlight">\(\leq{0}\)</span>, so the <span class="math notranslate nohighlight">\(x\leq{5}\)</span> becomes:</p>
<div class="math notranslate nohighlight">
\[x - 5 \leq{0}\]</div>
<p>Once the constraints have been converted, the next step is to start charging a penalty for violating them. Since we’re trying to minimize <span class="math notranslate nohighlight">\(f(x)\)</span>, we need to <em>add value</em> when the constraint is violated. If you are trying to maximize, the penalty will <em>subtract value</em>. With the constraint <span class="math notranslate nohighlight">\(x-5\leq{0}\)</span> we need a penalty that is:</p>
<ul class="simple">
<li><p><strong>Constraint satisfied</strong>: 0 when <span class="math notranslate nohighlight">\(x-5\leq{0}\)</span></p></li>
<li><p><strong>Constraint violated</strong>: postive when <span class="math notranslate nohighlight">\(x-5&gt;0\)</span>.</p></li>
</ul>
<p>This can be done by using a penalty <span class="math notranslate nohighlight">\(P\left(x\right)\)</span> of the form:</p>
<div class="math notranslate nohighlight" id="equation-eqn-quad-penalty">
<span class="eqno">(71)<a class="headerlink" href="#equation-eqn-quad-penalty" title="Link to this equation">#</a></span>\[P(x) = \max\left(0,x-5\right)^{2}\]</div>
<p>Eqn. <a class="reference internal" href="#equation-eqn-quad-penalty">(71)</a> is a quadratic penalty (loss) function. This approach works for equality constraints as well. For example, suppose we had the constraint <span class="math notranslate nohighlight">\(h(x) = c\)</span>, where <span class="math notranslate nohighlight">\(c\)</span> is a constant. We can convert this type of constraint into a penalty of the form:</p>
<div class="math notranslate nohighlight" id="equation-eqn-equality-constraint-pm">
<span class="eqno">(72)<a class="headerlink" href="#equation-eqn-equality-constraint-pm" title="Link to this equation">#</a></span>\[P(x) = \left(h(x) - c\right)^{2}\]</div>
<p>The lowest penalty value in <a class="reference internal" href="#equation-eqn-equality-constraint-pm">(72)</a> will occur when <span class="math notranslate nohighlight">\(h(x) = c\)</span>; thus, we satisfy the constraint. Once we have converted the constraints into penalty functions, we add all the penalty functions to the original objective function <span class="math notranslate nohighlight">\(f(x) + \lambda\cdot{P(x)}\)</span> and minimize the total function (objective plus penalties). For example, the original problem in <a class="reference internal" href="#equation-eqn-example-pmethod">(70)</a> becomes:</p>
<div class="math notranslate nohighlight" id="equation-eqn-final-penality-form">
<span class="eqno">(73)<a class="headerlink" href="#equation-eqn-final-penality-form" title="Link to this equation">#</a></span>\[\arg \min_{x} \left(\frac{100}{x} +\lambda\cdot\max\left(0,x-5\right)^{2}\right)\]</div>
<p>where <span class="math notranslate nohighlight">\(\lambda\)</span> is a <em>hyper-parameter</em> (a parameter associated with the method, not the problem) that is adjusted during the process to estimate the unknown value of <span class="math notranslate nohighlight">\(x\)</span> according to the policy (<a class="reference internal" href="#obs-lambda-policy">Observation 6</a>):</p>
<div class="proof observation admonition" id="obs-lambda-policy">
<p class="admonition-title"><span class="caption-number">Observation 6 </span> (<span class="math notranslate nohighlight">\(\lambda\)</span>-policy penalty method)</p>
<section class="observation-content" id="proof-content">
<p>For a penalty method, we start with a small <span class="math notranslate nohighlight">\(\lambda\)</span> and repeat the <span class="math notranslate nohighlight">\(x\)</span> estimation problem with larger and larger values of <span class="math notranslate nohighlight">\(\lambda\)</span>. This makes constraint violation more expensive for each subsequent refinement of the estimate of <span class="math notranslate nohighlight">\(x\)</span>.</p>
</section>
</div><p>With a penalty method, we can choose any value for the starting value of <span class="math notranslate nohighlight">\(x\)</span>. Let’s do an example penalty method calculation that uses a <a class="reference external" href="https://en.wikipedia.org/wiki/Evolutionary_algorithm">simple evolutionary algorithm</a> to generate new guesses for <span class="math notranslate nohighlight">\(x\)</span> (<a class="reference internal" href="#example-quad-penalty-method">Example 22</a>):</p>
<div class="proof example dropdown admonition" id="example-quad-penalty-method">
<p class="admonition-title"><span class="caption-number">Example 22 </span> (Penalty method example)</p>
<section class="example-content" id="proof-content">
<p>Solve the minimization problem for <span class="math notranslate nohighlight">\(x\)</span> given by Eqn. <a class="reference internal" href="#equation-eqn-example-pmethod">(70)</a> using a quadratic penalty method for an initial guess of <span class="math notranslate nohighlight">\(x = 20\)</span> for <span class="math notranslate nohighlight">\(\lambda = 1e3,1e6,1e9\)</span>.</p>
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="s">&quot;&quot;&quot;</span>
<span class="s">    _loss(x::Float64; λ::Float64 = 1.0) -&gt; Float64</span>

<span class="s">Loss function for penalty method example. </span>
<span class="s">&quot;&quot;&quot;</span>
<span class="k">function</span><span class="w"> </span><span class="n">_loss</span><span class="p">(</span><span class="n">x</span><span class="o">::</span><span class="kt">Float64</span><span class="p">;</span><span class="w"> </span><span class="n">λ</span><span class="o">::</span><span class="kt">Float64</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">1.0</span><span class="p">)</span><span class="o">::</span><span class="kt">Float64</span>

<span class="w">    </span><span class="c"># compute the f(x) and the penalty -</span>
<span class="w">    </span><span class="n">f</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">100.0</span><span class="o">/</span><span class="n">x</span><span class="p">;</span>
<span class="w">    </span><span class="n">P</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">max</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mf">5.0</span><span class="p">))</span><span class="o">^</span><span class="mi">2</span><span class="p">;</span>

<span class="w">    </span><span class="c"># return -</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">f</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">λ</span><span class="o">*</span><span class="n">P</span><span class="p">;</span>
<span class="k">end</span>

<span class="s">&quot;&quot;&quot;</span>
<span class="s">    main() -&gt; Float64</span>
<span class="s">Runs an evolutionary algorithm to estimate x̂ (min of loss function). </span>
<span class="s">&quot;&quot;&quot;</span>
<span class="k">function</span><span class="w"> </span><span class="n">main</span><span class="p">()</span><span class="o">::</span><span class="kt">Float64</span>

<span class="w">    </span><span class="c"># initialize -</span>
<span class="w">    </span><span class="n">Λ</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="mf">1e3</span><span class="p">,</span><span class="mf">1e6</span><span class="p">,</span><span class="mf">1e9</span><span class="p">];</span>
<span class="w">    </span><span class="n">xₒ</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">20.0</span><span class="p">;</span>
<span class="w">    </span><span class="n">best_loss</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">Inf</span><span class="p">;</span>
<span class="w">    </span><span class="n">x̂</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">xₒ</span><span class="p">;</span>
<span class="w">    </span><span class="n">max_number_of_iterations</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1000</span><span class="p">;</span>

<span class="w">    </span><span class="c"># use a simple evolutionary algorithm -</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="n">λ</span><span class="w"> </span><span class="o">∈</span><span class="w"> </span><span class="n">Λ</span>

<span class="w">        </span><span class="n">x′</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x̂</span><span class="w"> </span><span class="c"># initialize the current x, with the best value we have so far</span>

<span class="w">        </span><span class="c"># refine our best guess</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="n">_</span><span class="w"> </span><span class="o">∈</span><span class="w"> </span><span class="mi">1</span><span class="o">:</span><span class="n">max_number_of_iterations</span>
<span class="w">            </span>
<span class="w">            </span><span class="c"># compute the loss -</span>
<span class="w">            </span><span class="n">l</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">_loss</span><span class="p">(</span><span class="n">x′</span><span class="p">,</span><span class="w"> </span><span class="n">λ</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">λ</span><span class="p">);</span>
<span class="w">            </span>
<span class="w">            </span><span class="c"># is this loss *better* than our best solution so far?</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">l</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">best_loss</span><span class="p">)</span>
<span class="w">                </span><span class="n">x̂</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x′</span><span class="p">;</span>
<span class="w">                </span><span class="n">best_loss</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">l</span><span class="p">;</span>
<span class="w">            </span><span class="k">end</span>

<span class="w">            </span><span class="c"># generate a new guess for x -</span>
<span class="w">            </span><span class="n">x′</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x̂</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mf">0.1</span><span class="o">*</span><span class="n">randn</span><span class="p">()</span>
<span class="w">        </span><span class="k">end</span>
<span class="w">    </span><span class="k">end</span>

<span class="w">    </span><span class="c"># return -</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">x̂</span><span class="p">;</span>
<span class="k">end</span>

<span class="c"># call the main -</span>
<span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">main</span><span class="p">();</span>
</pre></div>
</div>
<p><strong>source</strong>: <a class="reference external" href="https://github.com/varnerlab/CHEME-1800-4800-Course-Repository-S23/tree/main/examples/unit-3-examples/penalty_method">Source for penalty method example</a></p>
</section>
</div></section>
<section id="barrier-functions">
<h4>Barrier functions<a class="headerlink" href="#barrier-functions" title="Link to this heading">#</a></h4>
<p>Let’s rethink the problem shown in Eqn. <a class="reference internal" href="#equation-eqn-example-pmethod">(70)</a>. Suppose, instead of developing the penality function shown in Eqn. <a class="reference internal" href="#equation-eqn-quad-penalty">(71)</a> to minimize <span class="math notranslate nohighlight">\(f(x)\)</span>, for a ccontraint of the form <span class="math notranslate nohighlight">\(g(x)\leq{0}\)</span> we developed a barrier function:</p>
<div class="math notranslate nohighlight" id="equation-eqn-barrier-function">
<span class="eqno">(74)<a class="headerlink" href="#equation-eqn-barrier-function" title="Link to this equation">#</a></span>\[B\left(x\right) = - \frac{1}{g(x)}\]</div>
<p>As the <span class="math notranslate nohighlight">\(g(x)\rightarrow{0}\)</span>, i.e., we approach constraint violation, the value of <span class="math notranslate nohighlight">\(B\left(x\right)\rightarrow\infty\)</span>. In a similar way to penality approach shown in Eqn. <a class="reference internal" href="#equation-eqn-final-penality-form">(73)</a>, we could augment the objective (loss) function:</p>
<div class="math notranslate nohighlight" id="equation-eqn-final-barrier-method">
<span class="eqno">(75)<a class="headerlink" href="#equation-eqn-final-barrier-method" title="Link to this equation">#</a></span>\[\arg \min_{x}\left(f(x) -\frac{1}{\lambda}\cdot{B(x)}\right)\]</div>
<p>The challenge of the barrier method shown in Eqn. <a class="reference internal" href="#equation-eqn-final-barrier-method">(75)</a> is selecting a starting point. The initial guess of the <span class="math notranslate nohighlight">\(x\)</span> must be inside the barrier. If this is true, we adjust the hyperparameter <span class="math notranslate nohighlight">\(\lambda\)</span> using the policy (<a class="reference internal" href="#obs-barrier-method-lambda-policy">Observation 7</a>):</p>
<div class="proof observation admonition" id="obs-barrier-method-lambda-policy">
<p class="admonition-title"><span class="caption-number">Observation 7 </span> (<span class="math notranslate nohighlight">\(\lambda\)</span>-policy barrier method)</p>
<section class="observation-content" id="proof-content">
<p>For a barrier method, we start with a large <span class="math notranslate nohighlight">\(\lambda\)</span> and repeat the <span class="math notranslate nohighlight">\(x\)</span> estimation problem with smaller and smaller values of <span class="math notranslate nohighlight">\(\lambda\)</span>. This allows us to solve the problem near the solution with limited input from the barrier. When
we gradually reduce <span class="math notranslate nohighlight">\(\lambda\)</span>, using our previous solution as a starting point, we move closer and closer to the barrier.</p>
</section>
</div><p>Barrier methods have on critical pathology (<a class="reference internal" href="#remark-barrier-method">Remark 5</a>):</p>
<div class="proof remark admonition" id="remark-barrier-method">
<p class="admonition-title"><span class="caption-number">Remark 5 </span> (Barrier method pathology)</p>
<section class="remark-content" id="proof-content">
<p>If, for some reason, we jump over the barrier, e.g., we start outside the feasible region (the set of <span class="math notranslate nohighlight">\(x\)</span> values allowed by the constraints), or during the calculation, we generate a candidate solution outside the barrier, this approach can fail.</p>
</section>
</div><p>Let’s do an example barrier method calculation that uses a <a class="reference external" href="https://en.wikipedia.org/wiki/Evolutionary_algorithm">simple evolutionary algorithm</a> to generate new guesses for <span class="math notranslate nohighlight">\(x\)</span> (<a class="reference internal" href="#example-barrier-method">Example 23</a>):</p>
<div class="proof example dropdown admonition" id="example-barrier-method">
<p class="admonition-title"><span class="caption-number">Example 23 </span> (Barrier method example)</p>
<section class="example-content" id="proof-content">
<p>Solve the minimization problem for <span class="math notranslate nohighlight">\(x\)</span> given by Eqn. <a class="reference internal" href="#equation-eqn-example-pmethod">(70)</a> using a barrier method for an initial guess of <span class="math notranslate nohighlight">\(x = 2.0\)</span> for <span class="math notranslate nohighlight">\(\lambda = 10,1,0.1\)</span>.</p>
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="s">&quot;&quot;&quot;</span>
<span class="s">    _loss(x::Float64; λ::Float64 = 1.0) -&gt; Float64</span>

<span class="s">Loss function for barrier method example. </span>
<span class="s">&quot;&quot;&quot;</span>
<span class="k">function</span><span class="w"> </span><span class="n">_loss</span><span class="p">(</span><span class="n">x</span><span class="o">::</span><span class="kt">Float64</span><span class="p">;</span><span class="w"> </span><span class="n">λ</span><span class="o">::</span><span class="kt">Float64</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">1.0</span><span class="p">)</span><span class="o">::</span><span class="kt">Float64</span>

<span class="w">    </span><span class="c"># compute the f(x) and the penalty -</span>
<span class="w">    </span><span class="n">f</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">100.0</span><span class="o">/</span><span class="n">x</span><span class="p">;</span>
<span class="w">    </span><span class="n">B</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">-</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="mi">5</span><span class="p">)</span>

<span class="w">    </span><span class="c"># return -</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">f</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">λ</span><span class="p">)</span><span class="o">*</span><span class="n">B</span><span class="p">;</span>
<span class="k">end</span>

<span class="s">&quot;&quot;&quot;</span>
<span class="s">    main() -&gt; Float64</span>
<span class="s">Runs an evolutionary algorithm to estimate x̂ (min of loss function). </span>
<span class="s">&quot;&quot;&quot;</span>
<span class="k">function</span><span class="w"> </span><span class="n">main</span><span class="p">()</span><span class="o">::</span><span class="kt">Float64</span>

<span class="w">    </span><span class="c"># initialize -</span>
<span class="w">    </span><span class="n">Λ</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="mi">10</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.01</span><span class="p">];</span>
<span class="w">    </span><span class="n">xₒ</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">2.0</span><span class="p">;</span>
<span class="w">    </span><span class="n">best_loss</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">Inf</span><span class="p">;</span>
<span class="w">    </span><span class="n">x̂</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">xₒ</span><span class="p">;</span>
<span class="w">    </span><span class="n">max_number_of_iterations</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1000</span><span class="p">;</span>

<span class="w">    </span><span class="c"># use a simple evolutionary algorithm -</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="n">λ</span><span class="w"> </span><span class="o">∈</span><span class="w"> </span><span class="n">Λ</span>

<span class="w">        </span><span class="n">x′</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x̂</span><span class="w"> </span><span class="c"># initialize the current x, with the best value we have so far</span>

<span class="w">        </span><span class="c"># refine our best guess</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="n">_</span><span class="w"> </span><span class="o">∈</span><span class="w"> </span><span class="mi">1</span><span class="o">:</span><span class="n">max_number_of_iterations</span>
<span class="w">            </span>
<span class="w">            </span><span class="c"># compute the loss -</span>
<span class="w">            </span><span class="n">l</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">_loss</span><span class="p">(</span><span class="n">x′</span><span class="p">,</span><span class="w"> </span><span class="n">λ</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">λ</span><span class="p">);</span>
<span class="w">            </span>
<span class="w">            </span><span class="c"># is this loss *better* than our best solution so far?</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">l</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">best_loss</span><span class="p">)</span>
<span class="w">                </span><span class="n">x̂</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x′</span><span class="p">;</span>
<span class="w">                </span><span class="n">best_loss</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">l</span><span class="p">;</span>
<span class="w">            </span><span class="k">end</span>

<span class="w">            </span><span class="c"># generate a new guess for x -</span>
<span class="w">            </span><span class="n">x′</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x̂</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mf">0.1</span><span class="o">*</span><span class="n">randn</span><span class="p">()</span>
<span class="w">        </span><span class="k">end</span>
<span class="w">    </span><span class="k">end</span>

<span class="w">    </span><span class="c"># return -</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">x̂</span><span class="p">;</span>
<span class="k">end</span>

<span class="c"># call the main -</span>
<span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">main</span><span class="p">();</span>
</pre></div>
</div>
<p><strong>source</strong>: <a class="reference external" href="https://github.com/varnerlab/CHEME-1800-4800-Course-Repository-S23/tree/main/examples/unit-3-examples/barrier_method">Source for barrier method example</a></p>
</section>
</div></section>
<section id="application-of-penalty-and-barrier-methods">
<h4>Application of penalty and barrier methods<a class="headerlink" href="#application-of-penalty-and-barrier-methods" title="Link to this heading">#</a></h4>
<p>In the context of statistical modeling, penalty, and barrier methods are often used to regularize the model, which means imposing constraints on the model parameters to prevent overfitting and improve the model’s generalization ability</p>
<p>Several different types of penalty methods are commonly used in statistical modeling, including:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Ridge_regression">Ridge regression</a> adds a penalty term to the loss function, which is the sum of the squares of the model parameters. This has the effect of shrinking the parameters towards zero and can help reduce the model’s variance.</p></li>
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Lasso_(statistics)">Lasso regression</a> adds a penalty term to the loss function, which is the sum of the absolute values of the model parameters. This has the effect of setting some of the parameters to zero, which can help to select a subset of essential features and reduce the complexity of the model.</p></li>
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Lasso_(statistics)#Group_lasso">Group Lasso</a> is similar to <a class="reference external" href="https://en.wikipedia.org/wiki/Lasso_(statistics)">lasso regression</a> but allows the user to group variables together and applies the penalty to the group rather than to each variable.</p></li>
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Lasso_(statistics)#Elastic_net">Elastic net</a> combines the ridge and lasso regression penalties and allows users to tune the balance between the two penalties.</p></li>
</ul>
<p>Penalty methods can be combined with <a class="reference external" href="https://optimization.cbe.cornell.edu/index.php?title=Main_Page">optimization algorithms</a> to find the best (optimal) values of the model parameters that minimize the loss function subject to the penalty constraints. These methods are often used in situations with many predictors, and the goal is to select a parsimonious model with a small number of essential features.</p>
<hr class="docutils" />
<!-- ## Clustering and classification
Cluster analysis groups objects such that items in the same group (called a cluster) are more similar (in some sense) to each other than to those in different clusters. Cluster analysis is an essential task of exploratory data analysis and a common technique for statistical data analysis, used in many fields, including pattern recognition, image analysis, information retrieval, bioinformatics, data compression, computer graphics, and machine learning.

k-means clustering is a method of vector quantization, originally taken from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster. 

````{prf:definition} k-means clustering
:label: defn-kmeans-problem

Given a set of observations $\left(\mathbf{x}_{1},\mathbf{x}_{2},\dots,\mathbf{x}_{d}\right)$, where each observation is a $n$-dimensional vector, k-means clustering partitions the $d$-observations into $k\leq{d}$ sets $\mathcal{S} = \left\{\mathcal{S}_{1}, \mathcal{S}_{2},\dots,\mathcal{S}_{k}\right\}$ so as to minimize the within-cluster sum of squares (WCSS):

```{math}
:label: eqn-k-means-clustering
\text{arg}\min_{\mathcal{S}} \sum_{i=1}^{k}\sum_{\mathbf{x}\in\mathcal{S}_{i}}||\mathbf{x}-\mu_{i}||^{2}
```

where $\mu_{i}$ denotes the mean of the points in set $\mathcal{S}_{i}$. 

````

--- -->
</section>
</section>
</section>
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h2>
<p>In this lecture, we introduced the learning problem, and <a class="reference external" href="https://en.wikipedia.org/wiki/Ordinary_least_squares">Ordinary least squares</a>. We started the lecture by introducing linear regression models, and the learning problem, we then explored two types of common ordinary least squares problems:</p>
<ul class="simple">
<li><p><a class="reference internal" href="#content-references-ols-general-introduction"><span class="std std-ref">Linear regression models</span></a> is a statistical model used to analyze the relationship between a dependent variable (also known as the response variable) and one or more independent variables (also known as explanatory variables or predictors) that are believed to influence the dependent variable.</p></li>
<li><p><a class="reference internal" href="#content-references-ols-unconstrained-problem"><span class="std std-ref">Unconstrained regression problems</span></a>. In unconstrained problems, the regression model parameters can take on any values and are not limited in any way.</p></li>
<li><p><a class="reference internal" href="#content-references-ols-constrained-problem"><span class="std std-ref">Constrained regression problems</span></a>. In constrained problems, the regression model parameters can be bound in some way, e.g., by physical reasoning or by some relationships between the parameters.</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "julia-1.10"
        },
        kernelOptions: {
            name: "julia-1.10",
            path: "./unit-3-learning"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'julia-1.10'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="learning-landing.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">The Learning Problem: Models, Learning, and Optimization</p>
      </div>
    </a>
    <a class="right-next"
       href="lp.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Linear Programming</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression-models">Linear regression models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sum-of-squared-errors-loss-function">Sum of squared errors loss function</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#unconstrained-regression-problems">Unconstrained regression problems</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#square-regression-problems">Square regression problems</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overdetermined-regression-problems">Overdetermined regression problems</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#computation-of-the-overdetermined-matrix-inverse">Computation of the overdetermined matrix inverse</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#underdetermined-regression-problems">Underdetermined regression problems</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#least-norm-solutions">Least-norm solutions</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#computation-of-the-underdetermined-matrix-inverse">Computation of the underdetermined matrix inverse</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#constrained-regression-problems">Constrained regression problems</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lagrange-multipliers">Lagrange multipliers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#penalty-methods">Penalty methods</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#quadratic-penalty-functions">Quadratic penalty functions</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#barrier-functions">Barrier functions</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#application-of-penalty-and-barrier-methods">Application of penalty and barrier methods</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Varner
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>